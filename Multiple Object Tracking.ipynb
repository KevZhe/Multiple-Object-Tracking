{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90821e79",
   "metadata": {},
   "source": [
    "# Multiple Object Tracking\n",
    "\n",
    "Multi-Object Tracking (MOT) is a core visual ability that humans poses to perform kinetic tasks and coordinate other tasks. The AI community has recognized the importance of MOT via a series of [competitions](https://motchallenge.net). \n",
    "\n",
    " The ability to reason even in the absence of perception input task was highlighted in Lecture 1 using a document camera and a canopy type of occlusion where an object moves below it. In this assignment, the object class is `ball` and the ability to reason over time will be demonstrated using [Kalman Filters](https://en.wikipedia.org/wiki/Kalman_filter). There will be two cases of occlusion: occlusion by a different object and occlusion by the same object (typical case of the later is on tracking people in crowds). \n",
    "\n",
    "## Task 1: Understand the problem and setup environment (20 points)\n",
    "\n",
    "The problem is best described using this explanatory video below of the raw source files of this assignment:\n",
    "\n",
    "1. [Single object tracking](https://github.com/sseshadr/auvsi-cv-all/blob/master/objectTracking/examples/ball.mp4)\n",
    "2. [Multi-object tracking](https://github.com/sseshadr/auvsi-cv-all/blob/master/objectTracking/examples/multiObject.avi)\n",
    "\n",
    "```{eval-rst}\n",
    ".. youtube:: 0jAC9sMQQuM\n",
    "```\n",
    "\n",
    "The associated to the video github is [here](https://github.com/sseshadr/auvsi-cv-all). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9012bb",
   "metadata": {},
   "source": [
    "# Write up\n",
    "\n",
    "\n",
    "Object Detection is a computer vision task where segments of images or videos are taken and analyzed for the presence and precise location of certain instances of objects. On the other hand, object tracking is the abiity to predict the path and trajectory of an object in a video. In multi-object tracking, multiple objects will have to be tracked concurrently. \n",
    "The ability to track objects over time will rely on a set of measurements of the position of objects, however, when these measurements are sparsely missing, we need a way to reason about the position of the object over time. This can be done using Kalman filters, which is an algorithm that can be used to give estimates for unknown variabes given a set of recorded measurements. Kalman filters are recursive, and rely on keeping a state of the object which includes variables such as position, velocity, and acceleration. This state is adjusted over time as new measurements become available, but when new measurments are not readily available, Kalman filters can make predictions using this stored state. \n",
    "\n",
    "To demonstrate the ability for Kalman Filters to reason over time, they will be used to predict the trajectories of sports balls. In the first part of the assignment, we will be implementing an object detector using the OpenCV library and YOLOv3 model. Individual frames of the video will be passed into the model for sports ball detection, and and a bounding box will be drawn around the resulting detection that exhibits the highest confidence. To be able to demonstrate reasoning over time, the videos will exhibit occlusion of the balls. Two scenarios will be tested here: occulusion of the ball by a different object (piece of paper), and occusion by the same object (multiple balls crossing paths). Measurements of the centroids of successful detections will be extracted from the object detector. This will be useful in the second part of the assignment: object tracking. For this task, we will be running Kalman Filters to keep track of the state of each instance of sports ball in the video. These Kalman Filters will be updated at each frame depending on if there is a recorded measurement from the object detector. When the objects are occluded, the Kalman Filters can use the stored state to make predictions. These predictions at each frame will be recorded and superimposed onto the video, demonstrating how Kalman Filters can be used to track objects over time.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9131c4",
   "metadata": {},
   "source": [
    "## Task 2: Object Detector (40 points)\n",
    "\n",
    "In this task you will use a CNN-based object detector to bound box all `ball` instances in each frame. Because the educational value is  not object detection, you are allowed to use an object detector of your choice trained to distinguish the `ball` class. You are free to use a pre-trained model (eg on MS COCO that contains the class `sports ball` or train a model yourself.  Ensure that you explain thoroughly the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e643a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#load our list of classes\n",
    "classes_lst = []\n",
    "with open('yolov3.txt', 'r') as cf:\n",
    "    #read line by line and add class name to our list\n",
    "    lines = cf.readlines()\n",
    "    for line in lines:\n",
    "        classes_lst.append(line.strip())\n",
    "        \n",
    "#load the YOLOv3 deepnet\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "\n",
    "#takes output from deepnet and puts boundary boxes on the frame passed in\n",
    "def fetch_boxes_and_confidence(outputs, frame):\n",
    "    \n",
    "    #initialize list to store bounding boxes\n",
    "    boxes = []\n",
    "    \n",
    "    #initialize list to store our confidences\n",
    "    confidences = []\n",
    "    \n",
    "    #initialize list to store our class ids\n",
    "    cIDs = []\n",
    "    \n",
    "    #initialize list to store the centroids of our detections\n",
    "    center = []\n",
    "    \n",
    "    #fetch height and width of the frame for bounding box calculation\n",
    "    height_px, width_px, _ = frame.shape\n",
    "    \n",
    "    #make bounding boxes for each detection in each output\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            \n",
    "            #fetch scores from our detection\n",
    "            scores = detection[5:]\n",
    "            #find indice that has highest score \n",
    "            cID = np.argmax(scores)\n",
    "            #index back into scores to fetch confidence\n",
    "            confidence = scores[cID]\n",
    "            #only put bounding boxes on detections with confidence higher than 0.5\n",
    "            if confidence > 0.5:\n",
    "                \n",
    "                #fetch the center X and Y for each detection\n",
    "                center_X = int(detection[0] * width_px)\n",
    "                center_Y = int(detection[1] * height_px)\n",
    "                #fetch the width and height for each detection\n",
    "                width = int(detection[2] * width_px)\n",
    "                height = int(detection[3] * height_px)\n",
    "                \n",
    "                #calculate the bottom left corner of the box\n",
    "                x = int(center_X - width/2)\n",
    "                y = int(center_Y - height/2)\n",
    "                \n",
    "                #add the box dimensional information to our list\n",
    "                boxes.append([x,y,width,height])\n",
    "                \n",
    "                #store our confidence and class id\n",
    "                confidences.append(confidence)\n",
    "                cIDs.append(cID)\n",
    "                \n",
    "    #use non-max-suppresion on our boxes to limit detections\n",
    "    chosen_boxes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    #put remaining boundings boxes on image \n",
    "    for idx in chosen_boxes:\n",
    "        \n",
    "        #get class label\n",
    "        class_label = classes_lst[cIDs[idx]]\n",
    "        \n",
    "        #if not detecting sports ball, toss\n",
    "        if class_label != \"sports ball\":\n",
    "            continue\n",
    "            \n",
    "        #fetch box\n",
    "        b = boxes[idx]\n",
    "        #get coordinates and dimensional info for box\n",
    "        x,y,w,h = b[0], b[1], b[2], b[3]\n",
    "        \n",
    "        #store the centroid \n",
    "        center.append([x + w/2, y + h/2])\n",
    "        \n",
    "        #put bounding box on image along with relevant information \n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        cv2.putText(frame,class_label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.3, color = (0,0,255))\n",
    "    \n",
    "    return frame, center\n",
    "        \n",
    "#use YOLOv3 to run detection on file   \n",
    "def run_detection(filename):\n",
    "    #create a videocapture object for the file\n",
    "    capture = cv2.VideoCapture(filename)\n",
    "    \n",
    "    #store list of all centroids\n",
    "    centroids = []\n",
    "    \n",
    "    #fetch frame width and height to get frame size\n",
    "    frame_width = int(capture.get(3))\n",
    "    frame_height = int(capture.get(4))\n",
    "    frame_size = (frame_width,frame_height)\n",
    "    #initialize our video\n",
    "    vid = cv2.VideoWriter('%s_detection.avi' % filename[:-4], cv2.VideoWriter_fourcc(*'MJPG'),15, frame_size)\n",
    "    \n",
    "    #keep reading frames until no more frames can be read\n",
    "    while True:\n",
    "        \n",
    "        #read a frame\n",
    "        ret, frame = capture.read()\n",
    "        \n",
    "        #if we have anything to read\n",
    "        if ret:\n",
    "            \n",
    "            #create blob object from the frame\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "            \n",
    "            #initialze our input to the net with the blob\n",
    "            net.setInput(blob)\n",
    "            \n",
    "            #get outputlayers\n",
    "            layernames = net.getLayerNames()\n",
    "            outputlayers = [layernames[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "            \n",
    "            #net forward and get output (detections)\n",
    "            output = net.forward(outputlayers)\n",
    "            \n",
    "            #put bounding boxes on our image based on the net output\n",
    "            frame, center = fetch_boxes_and_confidence(output, frame)\n",
    "            \n",
    "            #store our centers from the output\n",
    "            centroids.append(center)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('frame', frame)\n",
    "            \n",
    "            #write the frame to a video\n",
    "            vid.write(frame)\n",
    "\n",
    "        else: \n",
    "            break\n",
    "            \n",
    "    # Release the capture\n",
    "    capture.release()\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return centroids\n",
    "\n",
    "#call our detection function on test videos\n",
    "centroids_ball = run_detection(\"oneball.mp4\")\n",
    "centroids_multiball = run_detection(\"multiball.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d74d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[905.5, 277.5]], [[907.0, 278.5]], [[905.5, 279.0]], [[905.5, 278.5]], [[906.5, 277.5]], [[905.5, 278.0]], [[906.5, 275.0]], [[906.5, 275.0]], [[906.5, 274.0]], [[892.5, 272.0]], [[861.5, 270.5]], [[830.5, 272.5]], [[808.0, 268.0]], [[782.5, 271.5]], [[758.5, 272.0]], [[740.0, 270.5]], [[712.5, 271.0]], [[689.5, 267.0]], [[666.5, 267.0]], [], [], [], [], [], [], [], [], [], [], [], [], [[358.0, 261.0]], [[342.0, 257.5]], [], [], [[301.5, 254.0]], [[281.5, 253.0]], [[263.5, 254.0]], [[242.5, 253.0]], [[224.0, 252.5]], [[203.0, 250.5]], [[185.5, 251.0]], [[163.5, 250.5]], [[145.0, 251.0]], [[129.5, 247.0]], [[113.5, 246.0]], [[96.0, 246.0]], [[82.0, 245.0]], [[67.0, 245.0]], [[53.5, 246.5]], [[45.5, 255.0]]]\n"
     ]
    }
   ],
   "source": [
    "#Centroids for 1 ball\n",
    "print(centroids_ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fb64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[621.5, 149.0]], [[621.5, 149.0]], [[621.5, 148.5]], [[622.0, 148.5]], [[619.5, 148.5]], [], [[603.0, 153.5]], [[585.5, 153.0]], [[564.0, 153.0]], [[539.5, 152.5]], [[518.0, 153.5], [26.0, 148.0]], [[494.5, 153.0], [46.0, 149.5]], [[472.5, 153.0], [71.0, 151.0]], [[451.0, 153.0], [104.5, 152.5]], [[430.5, 153.5], [136.0, 153.0]], [[409.0, 154.0], [163.5, 152.5]], [[389.0, 154.0], [193.0, 153.5]], [[368.0, 154.0], [219.5, 154.0]], [[349.0, 155.0], [250.5, 153.0]], [], [[309.5, 154.5]], [], [[364.5, 154.0], [269.5, 155.5]], [[391.5, 157.0], [253.5, 156.0]], [[421.0, 155.5], [234.5, 156.0]], [[449.0, 155.0], [218.0, 154.5]], [[475.5, 154.5], [198.0, 154.5]], [[503.5, 155.0], [182.0, 154.5]], [[531.0, 155.5], [163.5, 154.0]], [[556.0, 155.0], [145.5, 154.5]], [[128.5, 154.5], [580.5, 154.0]], [[108.5, 153.5], [596.0, 152.5]], [[92.5, 155.0]], [[74.5, 153.5], [619.5, 154.5]], [[56.5, 153.5]], [[44.0, 154.0]], [[34.5, 153.5]], [[26.5, 154.5]], [[19.5, 154.5]], [], []]\n"
     ]
    }
   ],
   "source": [
    "#Centroids for 2 balls\n",
    "print(centroids_multiball)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f6a92",
   "metadata": {},
   "source": [
    "## Task 3: Tracker (40 points)\n",
    "\n",
    "The detector outputs can be used to obtain the centroid(s) of the `ball` instances across time. You can assign a suitable starting state in the 1st frame of the video and obtain the predicted trajectory of the object during both visible and occluded frames. You need to superpose your predicted position of the object in each frame and the raw frame and store a sequence of all frames (generate a video).  Ensure that you explain thoroughly the code. \n",
    "\n",
    "```{note}\n",
    "You can use OpenCV (`import cv2`) for only the satellite parts of this assignment - Use numpy, or better, jax to code the Kalman filter. You need to submit the assignment either as a notebook URL or a Github URL. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fac2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our Kalman Filter Class\n",
    "class KalmanFilter():\n",
    "    \n",
    "    def __init__(self, X, dt):\n",
    "        \n",
    "        self.X = X\n",
    "        #initialize state transition matrix\n",
    "        self.F = np.array([[1,0,dt,0],\n",
    "                           [0,1,0,dt],\n",
    "                           [0,0,1,0],\n",
    "                           [0,0,0,1]])\n",
    "        #observation model\n",
    "        self.H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\n",
    "        #covariance of observation noise\n",
    "        self.R = np.identity(2)\n",
    "        #covariance of process noise\n",
    "        self.Q = np.identity(4)\n",
    "        #posteriori estimate covariance matrix\n",
    "        self.P = np.identity(4)\n",
    "        #control input model & control vector, none for this case\n",
    "        self.B = 0\n",
    "        self.U = 0\n",
    "        \n",
    "    def predict(self):\n",
    "        #predict state estimate\n",
    "        self.X = np.dot(self.F,self.X) + np.dot(self.B,self.U)\n",
    "        #predict estimate covariance\n",
    "        self.P = np.dot(np.dot(self.F, self.P),self.F.T) + self.Q\n",
    "\n",
    "    def update_KF(self, z):\n",
    "        #innovation covariance\n",
    "        S = np.dot(np.dot(self.H,self.P), self.H.T) + self.R\n",
    "        #calculate kalman gain\n",
    "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\n",
    "        #update estimate covariance matrix\n",
    "        self.P = np.dot(np.identity(len(K)) - np.dot(K,self.H), self.P)\n",
    "        #update state estimate\n",
    "        self.X = self.X + np.dot(K, z - np.dot(self.H,self.X))\n",
    "\n",
    "        \n",
    "#update our Kalman Filter parameters\n",
    "def update(kf1,curr_center, frame, n_balls, kf2 = None):\n",
    "    #if we only have 1 ball in image\n",
    "    if n_balls == 1: \n",
    "        \n",
    "        #fetch actual center of ball\n",
    "        curr_center = np.array(curr_center).T\n",
    "        \n",
    "        #initial predict \n",
    "        kf1.predict()  \n",
    "        \n",
    "        #if we have a centroid from detection to use then update kalman filter\n",
    "        if len(curr_center) != 0:\n",
    "            kf1.update_KF(curr_center)\n",
    "        \n",
    "        #fetch prediction after updating\n",
    "        pred_x, pred_y = kf1.X[0][0], kf1.X[1][0]\n",
    "        \n",
    "        #create coordinates based on prediction and draw circle on frame\n",
    "        coords = (int(pred_x), int(pred_y))\n",
    "        frame = cv2.circle(frame, coords, radius = 5, color = (0, 0, 255), thickness = 2)\n",
    "        \n",
    "        return kf1, None, frame\n",
    "    else: # if we have 2 balls to track\n",
    "        \n",
    "        #predict on both filters\n",
    "        kf1.predict()\n",
    "        kf2.predict()\n",
    "        \n",
    "        #if we have centroids to sue\n",
    "        if len(curr_center) != 0:\n",
    "            #for each (max 2) centroid update the correct kalman filter\n",
    "            for point in curr_center:\n",
    "                \n",
    "                #get one centroid\n",
    "                p = np.array([point[0], point[1]])\n",
    "                \n",
    "                #get predictions from kalman filters\n",
    "                center_1 = kf1.X[0][0], kf1.X[1][0]\n",
    "                center_2 = kf2.X[0][0], kf2.X[1][0]\n",
    "                \n",
    "                #calculate euclidean distance of current centroid to each prediction\n",
    "                euclid1 =  np.linalg.norm(p - center_1)\n",
    "                euclid2 = np.linalg.norm(p - center_2)\n",
    "                \n",
    "                c = np.array([point]).T\n",
    "                \n",
    "                #update the kalman filter that is closer to the centroid\n",
    "                if euclid1 < euclid2:\n",
    "                    kf1.update_KF(c)\n",
    "                else:\n",
    "                    kf2.update_KF(c)\n",
    "        \n",
    "        #fetch predictions from both filters and create coordinates\n",
    "        pred_x1, pred_y1 = kf1.X[0][0], kf1.X[1][0]\n",
    "        coords1 = (int(pred_x1), int(pred_y1))\n",
    "        pred_x2, pred_y2 = kf2.X[0][0], kf2.X[1][0]\n",
    "        coords2 = (int(pred_x2), int(pred_y2))\n",
    "        \n",
    "        #update our frame with circle drawn on each coordinate\n",
    "        frame = cv2.circle(frame, coords1, radius = 5, color = (0, 0, 255), thickness = 2)\n",
    "        frame = cv2.circle(frame, coords2, radius = 5, color = (0, 255, 0), thickness = 2)\n",
    "        \n",
    "        return kf1, kf2, frame   \n",
    "        \n",
    "#perform tracking on video for 1 or 2 using kalman filters \n",
    "def tracking(filename, centroids, n_balls):\n",
    "    \n",
    "    #create videocapture object from file\n",
    "    capture = cv2.VideoCapture(filename)\n",
    "    #fetch frame width and height to create frame size\n",
    "    frame_width = int(capture.get(3))\n",
    "    frame_height = int(capture.get(4))\n",
    "    frame_size = (frame_width,frame_height)\n",
    "    #create video object\n",
    "    vid = cv2.VideoWriter('%s_tracking.avi' % filename[:-4], cv2.VideoWriter_fourcc(*'MJPG'),15, frame_size)\n",
    "    \n",
    "    #if we only have 1 ball to track, initialize our kalman filter with the starting center position\n",
    "    if n_balls == 1:\n",
    "        #np array of position X, Y, velocity of X, and velocity of Y\n",
    "        X = np.array([[centroids[0][0][0]], [centroids[0][0][1]], [1], [1]])\n",
    "        kf1 = KalmanFilter(X, 1)\n",
    "        kf2 = None\n",
    "    else: #if we have two balls to track, initialize two kalman filters with initial center positions for first frame\n",
    "        X1 = np.array([[621.5], [149.0], [1], [1]])\n",
    "        kf1 = KalmanFilter(X1, 1)\n",
    "        X2 = np.array([[5.0], [148.0], [1], [1]])\n",
    "        kf2 = KalmanFilter(X2, 1)\n",
    "    \n",
    "    #counter for fetching centroids\n",
    "    i = 0\n",
    "    while True:\n",
    "        #read a frame from the video\n",
    "        ret, frame = capture.read()\n",
    "        #if we have a frame to read\n",
    "        if ret:\n",
    "            \n",
    "            #fetch center(s)\n",
    "            curr_center = centroids[i]  \n",
    "            \n",
    "            #update our kalman filters and get updated frame with tracker\n",
    "            kf1, kf2, frame = update(kf1, curr_center, frame,n_balls, kf2)\n",
    "            \n",
    "            #write frame into video\n",
    "            vid.write(frame)\n",
    "            i+=1\n",
    "                \n",
    "        else: \n",
    "            break\n",
    "    #Release the capture\n",
    "    capture.release()\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "#call our tracking function on test videos\n",
    "tracking(\"oneball.mp4\", centroids_ball, 1)\n",
    "tracking(\"multiball.avi\", centroids_multiball, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
